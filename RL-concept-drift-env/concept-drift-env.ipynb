{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Author: Andrea Pierré\n",
    "# License: MIT License\n",
    "\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import NamedTuple\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "import gymnasium as gym\n",
    "from gymnasium.envs.toy_text.frozen_lake import generate_random_map\n",
    "\n",
    "\n",
    "sns.set_theme()\n",
    "\n",
    "# %load_ext lab_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Params(NamedTuple):\n",
    "    total_episodes: int  # Total episodes\n",
    "    learning_rate: float  # Learning rate\n",
    "    gamma: float  # Discounting rate\n",
    "    epsilon: float  # Exploration probability\n",
    "    map_size: int  # Number of tiles of one side of the squared environment\n",
    "    seed: int  # Define a seed so that we get reproducible results\n",
    "    is_slippery: bool  # If true the player will move in intended direction with probability of 1/3 else will move in either perpendicular direction with equal probability of 1/3 in both directions\n",
    "    n_runs: int  # Number of runs\n",
    "    action_size: int  # Number of possible actions\n",
    "    state_size: int  # Number of possible states\n",
    "    proba_frozen: float  # Probability that a tile is frozen\n",
    "    savefig_folder: Path  # Root folder where plots are saved\n",
    "\n",
    "\n",
    "params = Params(\n",
    "    total_episodes=2000,\n",
    "    learning_rate=0.8,\n",
    "    gamma=0.95,\n",
    "    epsilon=0.1,\n",
    "    map_size=5,\n",
    "    seed=123,\n",
    "    is_slippery=False,\n",
    "    n_runs=20,\n",
    "    action_size=None,\n",
    "    state_size=None,\n",
    "    proba_frozen=0.9,\n",
    "    savefig_folder=Path(\"../../_static/img/tutorials/\"),\n",
    ")\n",
    "params\n",
    "\n",
    "# Set the seed\n",
    "rng = np.random.default_rng(params.seed)\n",
    "\n",
    "# Create the figure folder if it doesn't exists\n",
    "params.savefig_folder.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Qlearning:\n",
    "    def __init__(self, learning_rate, gamma, state_size, action_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.gamma = gamma\n",
    "        self.reset_qtable()\n",
    "\n",
    "    def update(self, state, action, reward, new_state):\n",
    "        \"\"\"Update Q(s,a):= Q(s,a) + lr [R(s,a) + gamma * max Q(s',a') - Q(s,a)]\"\"\"\n",
    "        delta = (\n",
    "            reward\n",
    "            + self.gamma * np.max(self.qtable[new_state, :])\n",
    "            - self.qtable[state, action]\n",
    "        )\n",
    "        q_update = self.qtable[state, action] + self.learning_rate * delta\n",
    "        return q_update\n",
    "\n",
    "    def reset_qtable(self):\n",
    "        \"\"\"Reset the Q-table.\"\"\"\n",
    "        self.qtable = np.zeros((self.state_size, self.action_size))\n",
    "\n",
    "\n",
    "class EpsilonGreedy:\n",
    "    def __init__(self, epsilon):\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def choose_action(self, action_space, state, qtable):\n",
    "        \"\"\"Choose an action `a` in the current world state (s).\"\"\"\n",
    "        # First we randomize a number\n",
    "        explor_exploit_tradeoff = rng.uniform(0, 1)\n",
    "\n",
    "        # Exploration\n",
    "        if explor_exploit_tradeoff < self.epsilon:\n",
    "            action = action_space.sample()\n",
    "\n",
    "        # Exploitation (taking the biggest Q-value for this state)\n",
    "        else:\n",
    "            # Break ties randomly\n",
    "            # If all actions are the same for this state we choose a random one\n",
    "            # (otherwise `np.argmax()` would always take the first one)\n",
    "            if np.all(qtable[state, :]) == qtable[state, 0]:\n",
    "                action = action_space.sample()\n",
    "            else:\n",
    "                action = np.argmax(qtable[state, :])\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_env():\n",
    "    rewards = np.zeros((params.total_episodes, params.n_runs))\n",
    "    steps = np.zeros((params.total_episodes, params.n_runs))\n",
    "    episodes = np.arange(params.total_episodes)\n",
    "    qtables = np.zeros((params.n_runs, params.state_size, params.action_size))\n",
    "    all_states = []\n",
    "    all_actions = []\n",
    "\n",
    "    for run in range(params.n_runs):  # Run several times to account for stochasticity\n",
    "        learner.reset_qtable()  # Reset the Q-table between runs\n",
    "\n",
    "        for episode in tqdm(\n",
    "            episodes, desc=f\"Run {run}/{params.n_runs} - Episodes\", leave=False\n",
    "        ):\n",
    "            state = env.reset(seed=params.seed)[0]  # Reset the environment\n",
    "            step = 0\n",
    "            done = False\n",
    "            total_rewards = 0\n",
    "\n",
    "            while not done:\n",
    "                action = explorer.choose_action(\n",
    "                            action_space=env.action_space, state=state, qtable=learner.qtable\n",
    "                        )\n",
    "\n",
    "                #stability ← CharacterizeBatch()\n",
    "\n",
    "\n",
    "                # Log all states and actions\n",
    "                all_states.append(state)\n",
    "                all_actions.append(action)\n",
    "\n",
    "                # Take the action (a) and observe the outcome state(s') and reward (r)\n",
    "                new_state, reward, terminated, truncated, info = env.step(action)\n",
    "                \n",
    "                print(reward)\n",
    "\n",
    "                done = terminated or truncated\n",
    "\n",
    "                learner.qtable[state, action] = learner.update(\n",
    "                    state, action, reward, new_state\n",
    "                )\n",
    "\n",
    "                total_rewards += reward\n",
    "                step += 1\n",
    "\n",
    "                # Our new state is state\n",
    "                state = new_state\n",
    "\n",
    "            # Log all rewards and steps\n",
    "            rewards[episode, run] = total_rewards\n",
    "            steps[episode, run] = step\n",
    "        qtables[run, :, :] = learner.qtable\n",
    "\n",
    "    return rewards, steps, episodes, qtables, all_states, all_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#map_sizes = [4, 7, 9, 11]\n",
    "\n",
    "map_size = 4\n",
    "res_all = pd.DataFrame()\n",
    "st_all = pd.DataFrame()\n",
    "\n",
    "\n",
    "env = gym.make(\n",
    "        \"FrozenLake-v1\",\n",
    "        is_slippery=params.is_slippery,\n",
    "        render_mode=\"rgb_array\",\n",
    "        desc=generate_random_map(\n",
    "            size=map_size, p=params.proba_frozen, seed=params.seed\n",
    "        ),\n",
    "    )\n",
    "\n",
    "params = params._replace(action_size=env.action_space.n)\n",
    "params = params._replace(state_size=env.observation_space.n)\n",
    "env.action_space.seed(\n",
    "        params.seed\n",
    ")  # Set the seed to get reproducible results when sampling the action space\n",
    "\n",
    "learner = Qlearning(\n",
    "        learning_rate=params.learning_rate,\n",
    "        gamma=params.gamma,\n",
    "        state_size=params.state_size,\n",
    "        action_size=params.action_size,\n",
    "    )\n",
    "\n",
    "explorer = EpsilonGreedy(\n",
    "        epsilon=params.epsilon,\n",
    "    )\n",
    "\n",
    "print(f\"Map size: {map_size}x{map_size}\")\n",
    "rewards, steps, episodes, qtables, all_states, all_actions = run_env()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def characterize_batch(state,):\n",
    "    \n",
    "    return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def characterize_sample(state,):\n",
    "    \n",
    "    return \"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl-satellite-images",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
